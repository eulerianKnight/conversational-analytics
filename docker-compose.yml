version: '3.8'

services:
  # FastAPI Backend
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    container_name: analytics_backend
    ports:
      - "8000:8000"
    environment:
      - SECRET_KEY=${SECRET_KEY:-your-super-secret-key-change-this}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
      - SNOWFLAKE_SCHEMA=${SNOWFLAKE_SCHEMA}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
      - SNOWFLAKE_ROLE=${SNOWFLAKE_ROLE}
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
      - SMTP_SERVER=${SMTP_SERVER}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Streamlit Frontend
  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile
    container_name: analytics_frontend
    ports:
      - "8501:8501"
    environment:
      - API_BASE_URL=http://backend:8000
    depends_on:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis for caching (optional enhancement)
  redis:
    image: redis:7-alpine
    container_name: analytics_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes

volumes:
  redis_data:

networks:
  default:
    name: analytics_network

---
# .env.example
# Copy this file to .env and fill in your actual values

# Security
SECRET_KEY=your-super-secret-key-change-this-in-production

# Snowflake Configuration
SNOWFLAKE_ACCOUNT=your-account.region.snowflakecomputing.com
SNOWFLAKE_USER=your-username
SNOWFLAKE_PASSWORD=your-password
SNOWFLAKE_DATABASE=your-database
SNOWFLAKE_SCHEMA=your-schema
SNOWFLAKE_WAREHOUSE=your-warehouse
SNOWFLAKE_ROLE=your-role

# Claude API
CLAUDE_API_KEY=your-claude-api-key

# Email Configuration (for alerts)
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your-email@gmail.com
SMTP_PASSWORD=your-app-password

# Slack Configuration (for alerts)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK

---
# Makefile for easy development
# Makefile

.PHONY: help build up down logs clean test lint format

help: ## Show this help message
	@echo 'Usage: make [target]'
	@echo ''
	@echo 'Targets:'
	@egrep '^[a-zA-Z_-]+:.*?## .*$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  %-20s %s\n", $1, $2}'

build: ## Build Docker containers
	docker-compose build

up: ## Start all services
	docker-compose up -d

down: ## Stop all services
	docker-compose down

logs: ## Show logs for all services
	docker-compose logs -f

logs-backend: ## Show backend logs
	docker-compose logs -f backend

logs-frontend: ## Show frontend logs
	docker-compose logs -f frontend

clean: ## Remove containers, networks, and volumes
	docker-compose down -v --remove-orphans
	docker system prune -f

restart: ## Restart all services
	docker-compose restart

restart-backend: ## Restart backend service
	docker-compose restart backend

restart-frontend: ## Restart frontend service
	docker-compose restart frontend

shell-backend: ## Access backend container shell
	docker-compose exec backend bash

shell-frontend: ## Access frontend container shell
	docker-compose exec frontend bash

test: ## Run tests
	docker-compose exec backend pytest

lint: ## Run linting
	docker-compose exec backend flake8 app/

format: ## Format code
	docker-compose exec backend black app/

health: ## Check service health
	curl -f http://localhost:8000/health
	curl -f http://localhost:8501

setup: ## Initial setup - copy env file and build
	cp .env.example .env
	@echo "Please edit .env file with your configuration"
	@echo "Then run: make build && make up"

dev-backend: ## Run backend in development mode
	cd backend && uvicorn main:app --reload --host 0.0.0.0 --port 8000

dev-frontend: ## Run frontend in development mode
	cd frontend && streamlit run app.py --server.port 8501

install-dev: ## Install development dependencies
	cd backend && pip install -r requirements.txt
	cd frontend && pip install -r requirements.txt

---
# GitHub Actions workflow
# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        pip install pytest pytest-asyncio
    
    - name: Run tests
      run: |
        cd backend
        pytest tests/ -v
    
    - name: Run linting
      run: |
        cd backend
        flake8 app/ --max-line-length=100
    
    - name: Check code formatting
      run: |
        cd backend
        black --check app/

  build:
    needs: test
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Build Docker images
      run: |
        docker-compose build
    
    - name: Test Docker containers
      run: |
        docker-compose up -d
        sleep 30
        curl -f http://localhost:8000/health || exit 1
        curl -f http://localhost:8501 || exit 1
        docker-compose down

---
# README.md
# Agentic Analytics Solution

A comprehensive analytics platform that combines natural language processing with SQL querying, featuring supplier performance monitoring, sales forecasting, and intelligent alerting.

## ğŸš€ Features

- **Natural Language to SQL**: Convert plain English questions into SQL queries using Claude AI
- **Interactive Chat Interface**: Streamlit-based chat for conversational analytics
- **Supplier Performance Monitoring**: Track supplier metrics, delivery times, and performance KPIs
- **Sales Forecasting**: Historical analysis and trend prediction
- **Smart Alerting**: Configurable alerts with email and Slack notifications
- **Query Memory**: Context-aware conversations with query history
- **Visualization Engine**: Automatic chart generation using Plotly
- **User Management**: Authentication and role-based access control
- **Query Caching**: Performance optimization for large datasets

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚â”€â”€â”€â”€â”‚   FastAPI    â”‚â”€â”€â”€â”€â”‚   Snowflake     â”‚
â”‚   Frontend      â”‚    â”‚   Backend    â”‚    â”‚   Database      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                  â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SQLite DB     â”‚ â”‚ Claude API     â”‚ â”‚ Alert System  â”‚
    â”‚   (Memory &     â”‚ â”‚ (Text-to-SQL)  â”‚ â”‚ (Email/Slack) â”‚
    â”‚    Auth)        â”‚ â”‚                â”‚ â”‚               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Technology Stack

- **Backend**: FastAPI, Python 3.11+
- **Frontend**: Streamlit
- **Database**: Snowflake (primary), SQLite (metadata)
- **AI**: Claude API (Anthropic)
- **Visualization**: Plotly
- **Authentication**: JWT tokens
- **Containerization**: Docker & Docker Compose
- **Alerts**: SMTP email, Slack webhooks

## ğŸ“‹ Prerequisites

- Docker and Docker Compose
- Snowflake account with database access
- Claude API key from Anthropic
- Email account for SMTP (optional)
- Slack webhook URL (optional)

## ğŸš€ Quick Start

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd agentic-analytics
   ```

2. **Setup environment**
   ```bash
   make setup
   # Edit .env file with your configuration
   ```

3. **Build and start services**
   ```bash
   make build
   make up
   ```

4. **Access the application**
   - Frontend: http://localhost:8501
   - Backend API: http://localhost:8000
   - API Documentation: http://localhost:8000/docs

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file based on `.env.example`:

```env
# Security
SECRET_KEY=your-super-secret-key

# Snowflake
SNOWFLAKE_ACCOUNT=your-account.snowflakecomputing.com
SNOWFLAKE_USER=your-username
SNOWFLAKE_PASSWORD=your-password
SNOWFLAKE_DATABASE=your-database
SNOWFLAKE_SCHEMA=your-schema
SNOWFLAKE_WAREHOUSE=your-warehouse
SNOWFLAKE_ROLE=your-role

# Claude API
CLAUDE_API_KEY=your-claude-api-key

# Email (optional)
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your-email@gmail.com
SMTP_PASSWORD=your-app-password

# Slack (optional)
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK
```

## ğŸ“Š Database Schema

The solution works with a supply chain database containing:

- **PART**: Product catalog
- **SUPPLIER**: Supplier information
- **CUSTOMER**: Customer data
- **ORDERS**: Order headers
- **LINEITEM**: Order details (6M+ rows)
- **NATION**: Country reference
- **REGION**: Geographic regions
- **PARTSUPP**: Part-supplier relationships

## ğŸ¯ Key Use Cases

### Supplier Performance Monitoring
- Track delivery times and delays
- Monitor supplier reliability
- Analyze cost efficiency
- Regional performance comparison

### Sales Forecasting
- Historical trend analysis
- Seasonal pattern detection
- Revenue predictions
- Demand forecasting

### Interactive Analytics
- Natural language queries
- Conversational data exploration
- Automatic visualization
- Context-aware follow-ups

## ğŸ”§ Development

### Local Development

1. **Backend development**
   ```bash
   make dev-backend
   ```

2. **Frontend development**
   ```bash
   make dev-frontend
   ```

3. **Install dependencies**
   ```bash
   make install-dev
   ```

### Testing

```bash
make test
make lint
make format
```

### Docker Commands

```bash
make build          # Build containers
make up             # Start services
make down           # Stop services
make logs           # View logs
make restart        # Restart services
make clean          # Clean up containers
```

## ğŸ“š API Documentation

The FastAPI backend provides comprehensive API documentation:

- **Interactive docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Main Endpoints

- `POST /auth/login` - User authentication
- `POST /analytics/query` - Natural language query
- `GET /analytics/supplier-performance` - Supplier metrics
- `GET /analytics/sales-forecast` - Sales data
- `POST /alerts/` - Create alerts
- `GET /analytics/schema` - Database schema

## ğŸ”” Alert System

Configure intelligent alerts for:

- Revenue thresholds
- Supplier performance issues
- Inventory levels
- Customer behavior changes
- System anomalies

Notifications via:
- Email (SMTP)
- Slack webhooks
- Dashboard alerts

## ğŸ”’ Security Features

- JWT-based authentication
- Password hashing (bcrypt)
- SQL injection prevention
- Rate limiting
- Input validation
- Role-based access control

## ğŸ“ˆ Performance Optimization

- Query result caching
- Connection pooling
- Async operations
- Pagination for large datasets
- Background processing
- Index optimization

## ğŸš€ Deployment

### Production Deployment

1. **Configure production environment**
2. **Use production-grade secrets management**
3. **Setup SSL/TLS certificates**
4. **Configure monitoring and logging**
5. **Setup backup strategies**

### Scaling Considerations

- Load balancing for multiple instances
- Redis for distributed caching
- Database connection pooling
- Horizontal scaling with Docker Swarm/Kubernetes

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

For questions and support:

1. Check the documentation
2. Review existing issues
3. Create a new issue with detailed information
4. Contact the development team

## ğŸ”„ Roadmap

- [ ] Advanced ML models for forecasting
- [ ] Real-time data streaming
- [ ] Mobile app support
- [ ] Advanced visualization options
- [ ] Integration with BI tools
- [ ] Multi-tenant support
- [ ] Advanced analytics featuresuvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

---
# Frontend Dockerfile
# frontend/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8501

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:8501 || exit 1

# Run the application
CMD ["